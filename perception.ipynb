{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-20T22:42:10.814652Z",
     "start_time": "2024-04-20T22:42:10.810172Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(file_path, train_ratio=0.8, test_ratio=0.1, val_ratio=0.1):\n",
    "    data = np.genfromtxt(file_path, delimiter=',', skip_header=1)\n",
    "    states = data[:, :7]  # Extract the state information (pose, yaw, and laser data)\n",
    "    actions = data[:, 7:9]  # Extract the action information (linear and angular velocities)\n",
    "    timestamps = data[:, 11]  # Extract the timestamp information\n",
    "    key_presses = data[:, 10]  # Extract the key pressed information\n",
    "    target_distances = data[:, 9]  # Extract the target distance information\n",
    "\n",
    "    # Convert key presses to action types\n",
    "    action_types = []\n",
    "    for key in key_presses:\n",
    "        if key == 'w':\n",
    "            action_types.append(0)  # Move forward\n",
    "        elif key == 'a':\n",
    "            action_types.append(1)  # Turn left\n",
    "        elif key == 'd':\n",
    "            action_types.append(2)  # Turn right\n",
    "        elif key == 's':\n",
    "            action_types.append(3)  # Move backward\n",
    "        else:\n",
    "            action_types.append(4)  # Stop\n",
    "\n",
    "    # Calculate action durations based on timestamps\n",
    "    action_durations = []\n",
    "    for i in range(len(timestamps) - 1):\n",
    "        duration = timestamps[i + 1] - timestamps[i]\n",
    "        action_durations.append(duration)\n",
    "    action_durations.append(0)  # Set the duration of the last action as 0\n",
    "\n",
    "    max_laser_range = 3.5  # Assuming the maximum range is 3.5\n",
    "    states[:, 3:] = np.where(states[:, 3:] == float('inf'), max_laser_range, states[:, 3:])\n",
    "\n",
    "    # Normalize the states (positions, yaw, and laser data)\n",
    "    states_normalized = states.copy()\n",
    "    states_normalized[:, :2] /= np.max(states_normalized[:, :2])  # Normalize positions\n",
    "    states_normalized[:, 2] /= 2 * np.pi  # Normalize yaw angle\n",
    "    states_normalized[:, 3:] /= max_laser_range  # Normalize laser data (assuming maximum range is 3.5)\n",
    "\n",
    "    # Normalize the target distances\n",
    "    target_distances_normalized = target_distances / np.max(target_distances)\n",
    "\n",
    "    # Convert action types and action durations to numpy arrays\n",
    "    action_types = np.array(action_types)\n",
    "    action_durations = np.array(action_durations)\n",
    "\n",
    "    # Split the data into train, test, and validation sets\n",
    "    train_size = int(len(states_normalized) * train_ratio)\n",
    "    test_size = int(len(states_normalized) * test_ratio)\n",
    "\n",
    "    train_states, test_states, val_states = states_normalized[:train_size], states_normalized[train_size:train_size+test_size], states_normalized[train_size+test_size:]\n",
    "    train_actions, test_actions, val_actions = actions[:train_size], actions[train_size:train_size+test_size], actions[train_size+test_size:]\n",
    "    train_action_types, test_action_types, val_action_types = action_types[:train_size], action_types[train_size:train_size+test_size], action_types[train_size+test_size:]\n",
    "    train_action_durations, test_action_durations, val_action_durations = action_durations[:train_size], action_durations[train_size:train_size+test_size], action_durations[train_size+test_size:]\n",
    "    train_target_distances, test_target_distances, val_target_distances = target_distances_normalized[:train_size], target_distances_normalized[train_size:train_size+test_size], target_distances_normalized[train_size+test_size:]\n",
    "\n",
    "    return train_states, train_actions, train_action_types, train_action_durations, train_target_distances, \\\n",
    "        test_states, test_actions, test_action_types, test_action_durations, test_target_distances, \\\n",
    "        val_states, val_actions, val_action_types, val_action_durations, val_target_distances\n",
    "\n",
    "# Create the policy network\n",
    "def create_policy_network(state_dim, num_action_types):\n",
    "    inputs = layers.Input(shape=(state_dim,))\n",
    "    x = layers.Dense(128, activation='relu')(inputs)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    action_type_output = layers.Dense(num_action_types, activation='softmax')(x)\n",
    "    action_duration_output = layers.Dense(1, activation='relu')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[action_type_output, action_duration_output])\n",
    "    return model\n",
    "\n",
    "# Create the discriminator network\n",
    "def create_discriminator_network(state_dim, num_action_types):\n",
    "    state_inputs = layers.Input(shape=(state_dim,))\n",
    "    action_type_inputs = layers.Input(shape=(num_action_types,))  # Update the input shape\n",
    "    action_duration_inputs = layers.Input(shape=(1,))\n",
    "    x = layers.Concatenate()([state_inputs, action_type_inputs, action_duration_inputs])\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs=[state_inputs, action_type_inputs, action_duration_inputs], outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Create the dynamics model\n",
    "def create_dynamics_model(state_dim, num_action_types):\n",
    "    state_inputs = layers.Input(shape=(state_dim,))\n",
    "    action_type_inputs = layers.Input(shape=(num_action_types,))\n",
    "    action_duration_inputs = layers.Input(shape=(1,))\n",
    "    x = layers.Concatenate()([state_inputs, action_type_inputs, action_duration_inputs])\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    next_state_outputs = layers.Dense(state_dim)(x)\n",
    "    model = tf.keras.Model(inputs=[state_inputs, action_type_inputs, action_duration_inputs], outputs=next_state_outputs)\n",
    "    return model\n",
    "\n",
    "# Create the value network\n",
    "def create_value_network(state_dim):\n",
    "    inputs = layers.Input(shape=(state_dim,))\n",
    "    x = layers.Dense(128, activation='relu')(inputs)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Create the Q-networks\n",
    "def create_q_networks(state_dim, num_action_types):\n",
    "    state_inputs = layers.Input(shape=(state_dim,))\n",
    "    action_type_inputs = layers.Input(shape=(num_action_types,))\n",
    "    action_duration_inputs = layers.Input(shape=(1,))\n",
    "    x = layers.Concatenate()([state_inputs, action_type_inputs, action_duration_inputs])\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(inputs=[state_inputs, action_type_inputs, action_duration_inputs], outputs=outputs)\n",
    "    return model, model\n",
    "\n",
    "# Train the V-MAIL algorithm with SAC\n",
    "def train_v_mail_with_sac(train_states, train_actions, train_action_types, train_action_durations, train_target_distances,\n",
    "                          val_states, val_actions, val_target_distances,\n",
    "                          num_epochs, batch_size, state_dim, num_action_types, gamma=0.99, alpha=0.2, target_entropy=-2):\n",
    "    # Create the policy, discriminator, dynamics, value, and Q-networks\n",
    "    policy_model = create_policy_network(state_dim, num_action_types)\n",
    "    discriminator_model = create_discriminator_network(state_dim, num_action_types)\n",
    "    dynamics_model = create_dynamics_model(state_dim, num_action_types)\n",
    "    value_model = create_value_network(state_dim)\n",
    "    q_model1, q_model2 = create_q_networks(state_dim, num_action_types)\n",
    "\n",
    "    # Define the optimizers\n",
    "    policy_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "    dynamics_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "    value_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "    q_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "    alpha_optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "    # Entropy regularization coefficient\n",
    "    log_alpha = tf.Variable(tf.math.log(tf.cast(alpha, tf.float32)), trainable=True)\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Sample a batch of expert states, action types, action durations, and target distances\n",
    "        batch_indices = np.random.choice(len(train_states), size=batch_size).astype(int)\n",
    "        batch_expert_states = np.array(train_states)[batch_indices]\n",
    "        batch_expert_action_types = np.array(train_action_types)[batch_indices]\n",
    "        batch_expert_action_durations = np.array(train_action_durations)[batch_indices]\n",
    "        batch_expert_target_distances = np.array(train_target_distances)[batch_indices]\n",
    "\n",
    "\n",
    "        # Generate simulated action types and durations using the policy model\n",
    "        with tf.GradientTape() as tape:\n",
    "            batch_simulated_action_types, batch_simulated_action_durations = policy_model(batch_expert_states)\n",
    "            batch_simulated_states = dynamics_model([tf.convert_to_tensor(batch_expert_states), tf.convert_to_tensor(batch_simulated_action_types), tf.convert_to_tensor(batch_simulated_action_durations)])\n",
    "\n",
    "            # Convert batch_expert_action_types to one-hot encoding\n",
    "            batch_expert_action_types_one_hot = tf.one_hot(batch_expert_action_types, num_action_types)\n",
    "\n",
    "            # Convert batch_simulated_action_types to one-hot encoding\n",
    "            batch_simulated_action_types_one_hot = tf.one_hot(tf.argmax(batch_simulated_action_types, axis=1), num_action_types)\n",
    "\n",
    "            # Calculate the discriminator loss\n",
    "            expert_logits = discriminator_model([tf.convert_to_tensor(batch_expert_states), tf.convert_to_tensor(batch_expert_action_types_one_hot), tf.convert_to_tensor(batch_expert_action_durations)])\n",
    "            simulated_logits = discriminator_model([tf.convert_to_tensor(batch_simulated_states), tf.convert_to_tensor(batch_simulated_action_types_one_hot), tf.convert_to_tensor(batch_simulated_action_durations)])\n",
    "            discriminator_loss = tf.reduce_mean(tf.math.log(expert_logits) + tf.math.log(1 - simulated_logits))\n",
    "\n",
    "        # Update the discriminator model\n",
    "        discriminator_gradients = tape.gradient(discriminator_loss, discriminator_model.trainable_variables)\n",
    "        discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator_model.trainable_variables))\n",
    "\n",
    "        # Update the policy, dynamics, value, and Q-networks\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            batch_simulated_action_types, batch_simulated_action_durations = policy_model(batch_expert_states)\n",
    "            batch_simulated_states = dynamics_model([tf.convert_to_tensor(batch_expert_states), tf.convert_to_tensor(batch_simulated_action_types), tf.convert_to_tensor(batch_simulated_action_durations)])\n",
    "\n",
    "\n",
    "            # Calculate the Q-values\n",
    "            q_value1 = q_model1([tf.convert_to_tensor(batch_expert_states), tf.convert_to_tensor(batch_simulated_action_types), tf.convert_to_tensor(batch_simulated_action_durations)])\n",
    "            q_value2 = q_model2([tf.convert_to_tensor(batch_expert_states), tf.convert_to_tensor(batch_simulated_action_types), tf.convert_to_tensor(batch_simulated_action_durations)])\n",
    "            q_value = tf.minimum(q_value1, q_value2)\n",
    "\n",
    "            # Calculate the value targets\n",
    "            value_targets = q_value - tf.exp(log_alpha) * tf.reduce_sum(batch_simulated_action_types * tf.math.log(batch_simulated_action_types + 1e-8), axis=1, keepdims=True)\n",
    "\n",
    "            # Calculate the value loss\n",
    "            value_predictions = value_model(batch_expert_states)\n",
    "            value_loss = tf.reduce_mean(tf.square(value_predictions - value_targets))\n",
    "\n",
    "            # Calculate the Q-value targets\n",
    "            rewards = -batch_expert_target_distances  # Negative target distances as rewards\n",
    "            next_values = value_model(batch_simulated_states)\n",
    "            q_value_targets = rewards + gamma * next_values\n",
    "\n",
    "            # Calculate the Q-value loss\n",
    "            q_value_loss = tf.reduce_mean(tf.square(q_value1 - q_value_targets) + tf.square(q_value2 - q_value_targets))\n",
    "\n",
    "            # Calculate the policy loss\n",
    "            policy_loss = tf.reduce_mean(tf.exp(log_alpha) * tf.reduce_sum(batch_simulated_action_types * tf.math.log(batch_simulated_action_types + 1e-8), axis=1, keepdims=True) - q_value)\n",
    "\n",
    "            # Calculate the alpha loss\n",
    "            alpha_loss = -tf.reduce_mean(log_alpha * tf.stop_gradient(tf.reduce_sum(batch_simulated_action_types * tf.math.log(batch_simulated_action_types + 1e-8), axis=1, keepdims=True) - target_entropy))\n",
    "\n",
    "        # Update the policy model\n",
    "        policy_gradients = tape.gradient(policy_loss, policy_model.trainable_variables)\n",
    "        # print(\"Policy gradients:\", policy_gradients)\n",
    "        policy_optimizer.apply_gradients(zip(policy_gradients, policy_model.trainable_variables))\n",
    "\n",
    "        # Update the dynamics model\n",
    "        dynamics_gradients = tape.gradient(policy_loss, dynamics_model.trainable_variables)\n",
    "        # print(\"Dynamics gradients:\", dynamics_gradients)\n",
    "        if any(grad is not None for grad in dynamics_gradients):\n",
    "            dynamics_optimizer.apply_gradients(zip(dynamics_gradients, dynamics_model.trainable_variables))\n",
    "\n",
    "        # Update the value model\n",
    "        value_gradients = tape.gradient(value_loss, value_model.trainable_variables)\n",
    "        # print(\"Value gradients:\", value_gradients)\n",
    "        value_optimizer.apply_gradients(zip(value_gradients, value_model.trainable_variables))\n",
    "\n",
    "        # Update the Q-networks\n",
    "        q_gradients = tape.gradient(q_value_loss, q_model1.trainable_variables + q_model2.trainable_variables)\n",
    "        # print(\"Q-network gradients:\", q_gradients)\n",
    "        q_optimizer.apply_gradients(zip(q_gradients, q_model1.trainable_variables + q_model2.trainable_variables))\n",
    "\n",
    "        # Update the alpha value\n",
    "        alpha_gradients = tape.gradient(alpha_loss, [log_alpha])\n",
    "        # print(\"Alpha gradients:\", alpha_gradients)\n",
    "        alpha_optimizer.apply_gradients(zip(alpha_gradients, [log_alpha]))\n",
    "\n",
    "        # Print the training progress\n",
    "        if epoch % 100 == 0:\n",
    "            # Evaluate the model on the validation set\n",
    "            val_action_types, val_action_durations = policy_model.predict(val_states)\n",
    "            val_simulated_states = dynamics_model.predict([tf.convert_to_tensor(val_states), tf.convert_to_tensor(val_action_types), tf.convert_to_tensor(val_action_durations)])\n",
    "            val_rewards = -val_target_distances\n",
    "        \n",
    "            # Convert val_action_types to one-hot encoding\n",
    "            val_action_types_one_hot = tf.one_hot(tf.argmax(val_action_types, axis=1), num_action_types)\n",
    "        \n",
    "            val_q_values = q_model1.predict([tf.convert_to_tensor(val_states), tf.convert_to_tensor(val_action_types_one_hot), tf.convert_to_tensor(val_action_durations)])\n",
    "            val_value_predictions = value_model.predict(val_states)\n",
    "            val_loss = tf.reduce_mean(tf.square(val_value_predictions - val_rewards))\n",
    "        \n",
    "            print(f\"Epoch {epoch}: Discriminator Loss = {discriminator_loss:.4f}, Policy Loss = {policy_loss:.4f}, Value Loss = {value_loss:.4f}, Q-Value Loss = {q_value_loss:.4f}, Alpha Loss = {alpha_loss:.4f}, Validation Loss = {val_loss:.4f}\")\n",
    "        \n",
    "            # Convert val_action_types to numerical representations\n",
    "            val_action_types_numerical = tf.argmax(val_action_types, axis=1)\n",
    "        \n",
    "            # Evaluate the trained policy on the validation set\n",
    "            val_action_diff = np.mean(np.square(val_action_types_numerical - val_actions[:, 0]))  # Compare with the first dimension of val_actions\n",
    "            print(f\"Validation Action Difference: {val_action_diff:.4f}\")\n",
    "\n",
    "    return policy_model, dynamics_model, value_model, q_model1, q_model2"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 51ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 40ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 39ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 36ms/step\n",
      "Epoch 0: Discriminator Loss = -1.5992, Policy Loss = -0.3273, Value Loss = 0.1173, Q-Value Loss = 0.8648, Alpha Loss = 0.6312, Validation Loss = 0.0253\n",
      "Validation Action Difference: 7.1429\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step\n",
      "Epoch 100: Discriminator Loss = -12.6469, Policy Loss = 0.3220, Value Loss = 0.0006, Q-Value Loss = 0.0796, Alpha Loss = 0.6770, Validation Loss = 0.0973\n",
      "Validation Action Difference: 8.0000\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step\n",
      "Epoch 200: Discriminator Loss = nan, Policy Loss = 0.3416, Value Loss = 0.0017, Q-Value Loss = 0.0687, Alpha Loss = 0.6831, Validation Loss = 0.1035\n",
      "Validation Action Difference: 11.8571\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "Epoch 300: Discriminator Loss = nan, Policy Loss = 0.4312, Value Loss = 0.0012, Q-Value Loss = 0.0633, Alpha Loss = 0.6781, Validation Loss = 0.1601\n",
      "Validation Action Difference: 9.2857\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step\n",
      "Epoch 400: Discriminator Loss = nan, Policy Loss = 0.5110, Value Loss = 0.0017, Q-Value Loss = 0.0784, Alpha Loss = 0.6628, Validation Loss = 0.2109\n",
      "Validation Action Difference: 10.5714\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step\n",
      "Epoch 500: Discriminator Loss = nan, Policy Loss = 0.5548, Value Loss = 0.0014, Q-Value Loss = 0.0781, Alpha Loss = 0.6524, Validation Loss = 0.3222\n",
      "Validation Action Difference: 12.2143\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step\n",
      "Epoch 600: Discriminator Loss = nan, Policy Loss = 0.6541, Value Loss = 0.0017, Q-Value Loss = 0.0950, Alpha Loss = 0.6322, Validation Loss = 0.3856\n",
      "Validation Action Difference: 12.2143\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step\n",
      "Epoch 700: Discriminator Loss = nan, Policy Loss = 0.6393, Value Loss = 0.0003, Q-Value Loss = 0.0849, Alpha Loss = 0.6173, Validation Loss = 0.3925\n",
      "Validation Action Difference: 9.2857\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step\n",
      "Epoch 800: Discriminator Loss = nan, Policy Loss = 0.7104, Value Loss = 0.0003, Q-Value Loss = 0.0782, Alpha Loss = 0.5973, Validation Loss = 0.5004\n",
      "Validation Action Difference: 10.5714\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 27ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step\n",
      "Epoch 900: Discriminator Loss = nan, Policy Loss = 0.7146, Value Loss = 0.0003, Q-Value Loss = 0.0651, Alpha Loss = 0.5800, Validation Loss = 0.5129\n",
      "Validation Action Difference: 13.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set the file path for the collected data\n",
    "file_path = 'imitation_learning_dataset.csv'\n",
    "\n",
    "# Preprocess the data\n",
    "train_states, train_actions, train_action_types, train_action_durations, train_target_distances, \\\n",
    "    test_states, test_actions, test_action_types, test_action_durations, test_target_distances, \\\n",
    "    val_states, val_actions, val_action_types, val_action_durations, val_target_distances = preprocess_data(file_path)\n",
    "\n",
    "# Set the training parameters\n",
    "num_epochs = 1000\n",
    "batch_size = 64\n",
    "state_dim = 7  # Number of state variables (pose, yaw, and laser data)\n",
    "num_action_types = 5  # Number of action types (e.g., 'forward', 'left', 'right', 'backward', 'stop')\n",
    "\n",
    "# Train the V-MAIL algorithm with SAC\n",
    "policy_model, dynamics_model, value_model, q_model1, q_model2 = train_v_mail_with_sac(\n",
    "    train_states, train_actions, train_action_types, train_action_durations, train_target_distances,\n",
    "    val_states, val_actions, val_target_distances,\n",
    "    num_epochs, batch_size, state_dim, num_action_types\n",
    ")\n",
    "# Save the trained models\n",
    "policy_model.save('policy_model.h5')\n",
    "dynamics_model.save('dynamics_model.h5')\n",
    "value_model.save('value_model.h5')\n",
    "q_model1.save('q_model1.h5')\n",
    "q_model2.save('q_model2.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T22:44:45.592523Z",
     "start_time": "2024-04-20T22:42:14.314203Z"
    }
   },
   "id": "97a462b08d1914f4",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 658ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 48ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 46ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 33ms/step\n",
      "Test Loss: 0.2291\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 303ms/step\n",
      "Predicted Action: Move forward\n",
      "Predicted Action Duration: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained models on the test set\n",
    "test_action_types, test_action_durations = policy_model.predict(test_states)\n",
    "test_simulated_states = dynamics_model.predict([test_states, test_action_types, test_action_durations])\n",
    "test_rewards = -np.array(test_target_distances)\n",
    "test_q_values = q_model1.predict([test_states, test_action_types, test_action_durations])\n",
    "test_value_predictions = value_model.predict(test_states)\n",
    "test_loss = tf.reduce_mean(tf.square(test_value_predictions - test_rewards))\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Use the trained policy model for inference\n",
    "def predict_action(state):\n",
    "    action_type, action_duration = policy_model.predict(np.array([state]))\n",
    "    action_type = np.argmax(action_type[0])\n",
    "    action_duration = action_duration[0][0]\n",
    "    return action_type, action_duration\n",
    "\n",
    "# Example usage of the predict_action function\n",
    "current_state = test_states[0]  # Replace with the current state of the robot\n",
    "predicted_action_type, predicted_action_duration = predict_action(current_state)\n",
    "\n",
    "# Map the predicted action type to the corresponding action\n",
    "action_mapping = {\n",
    "    0: 'Move forward',\n",
    "    1: 'Turn left',\n",
    "    2: 'Turn right',\n",
    "    3: 'Move backward',\n",
    "    4: 'Stop'\n",
    "}\n",
    "\n",
    "predicted_action = action_mapping[predicted_action_type]\n",
    "print(f\"Predicted Action: {predicted_action}\")\n",
    "print(f\"Predicted Action Duration: {predicted_action_duration:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-20T22:45:21.121931Z",
     "start_time": "2024-04-20T22:45:19.858050Z"
    }
   },
   "id": "b2f7764b3456222d",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "960b04a292d15ac0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
